2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 16:49:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167e9ae198b0002, negotiated timeout = 10000
2018-12-26 16:49:28 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-26 16:49:28 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-26 16:49:29 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167e9ae198b0002 closed
2018-12-26 16:49:29 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167e9ae198b0002
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 16:49:30 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 16:49:30 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 61682.
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-e21398e7-0eef-4d32-a233-aef034750eca
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @4773ms
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/jobs/job,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/jobs/job/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/stage,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/stage/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/stages/pool,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/stages/pool/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/storage/rdd,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/storage/rdd/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/environment,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/environment/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/executors/threadDump,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/static,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/api,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@34448e6c{/jobs/job/kill,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@60e9df3c{/stages/stage/kill,null,AVAILABLE}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@1398309d{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @4924ms
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61703.
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:61703
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 61703, None)
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:61703 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 61703, None)
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 61703, None)
2018-12-26 16:49:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 61703, None)
2018-12-26 16:49:31 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@59942b48{/metrics/json,null,AVAILABLE}
2018-12-26 16:49:35 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 16:49:35 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545814175000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-26 17:48:28 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 17:48:28 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167e9e2b2d80001, negotiated timeout = 10000
2018-12-26 17:48:28 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-26 17:48:28 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-26 17:48:29 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167e9e2b2d80001 closed
2018-12-26 17:48:29 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167e9e2b2d80001
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 17:48:31 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 17:48:31 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 63579.
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-739f34ff-117e-4714-a50f-983905b946cb
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @6682ms
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/jobs/job,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/jobs/job/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/stage,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/stage/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/stages/pool,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/stages/pool/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/storage/rdd,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/storage/rdd/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/environment,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/environment/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/executors/threadDump,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/static,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/api,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@34448e6c{/jobs/job/kill,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@60e9df3c{/stages/stage/kill,null,AVAILABLE}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@32fdec40{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 17:48:32 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @6824ms
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 17:48:32 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63600.
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:63600
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 63600, None)
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:63600 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 63600, None)
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 63600, None)
2018-12-26 17:48:33 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 63600, None)
2018-12-26 17:48:33 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1869f114{/metrics/json,null,AVAILABLE}
2018-12-26 17:48:35 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 17:48:35 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545817715000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-26 17:54:44 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167e9e2b2d80004, negotiated timeout = 10000
2018-12-26 17:54:44 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-26 17:54:44 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167e9e2b2d80004 closed
2018-12-26 17:54:44 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167e9e2b2d80004
2018-12-26 17:54:45 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 17:54:46 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 63851.
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-a8c1b58a-d8ac-43f2-afc5-dcf0bb415eb3
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @3394ms
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7d0332e1{/jobs,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7a356a0d{/jobs/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs/job,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/job/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/stages,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/stages/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages/stage,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/stage/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/pool,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/pool/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/storage,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/storage/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage/rdd,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/rdd/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/environment,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/environment/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/executors,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/executors/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors/threadDump,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/static,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/api,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/jobs/job/kill,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/stages/stage/kill,null,AVAILABLE}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@3088660d{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @3506ms
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63860.
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:63860
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 63860, None)
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:63860 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 63860, None)
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 63860, None)
2018-12-26 17:54:46 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 63860, None)
2018-12-26 17:54:46 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@47b2e9e1{/metrics/json,null,AVAILABLE}
2018-12-26 17:54:46 [ WARN ] - [ org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66) ] spark.master should be set as local[n], n > 1 in local mode if you have receivers to get data, otherwise Spark jobs will not get resources to process the received data.
2018-12-26 17:54:50 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 17:54:50 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545818090000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-26 17:58:45 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar;C:\Users\junmei02\.IntelliJIdea2018.2\system\captureAgent\debugger-agent.jar
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 17:58:45 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@5cc126dc
2018-12-26 17:58:46 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 17:58:46 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 17:58:46 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167e9e2b2d80006, negotiated timeout = 10000
2018-12-26 17:58:46 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-26 17:58:46 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-26 17:58:46 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167e9e2b2d80006 closed
2018-12-26 17:58:46 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167e9e2b2d80006
2018-12-26 17:59:12 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 17:59:14 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 17:59:14 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 17:59:14 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 17:59:14 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 17:59:14 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 17:59:14 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 63956.
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-a56bfdea-ccc4-436c-b107-39b0b4c6778c
2018-12-26 17:59:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 17:59:16 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @40833ms
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7c0777b5{/jobs,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2489e84a{/jobs/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2a39aa2b{/jobs/job,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@62b93086{/jobs/job/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@254449bb{/stages,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4e642ee1{/stages/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@29ebbdf4{/stages/stage,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2fd954f{/stages/stage/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5731d3a{/stages/pool,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6a0f2853{/stages/pool/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1eff3cfb{/storage,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@70c69586{/storage/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6dc1dc69{/storage/rdd,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7e9f2c32{/storage/rdd/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19e0dffe{/environment,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5d4e13e1{/environment/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3e0fbeb5{/executors,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3976ebfa{/executors/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2676dc05{/executors/threadDump,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@b0a1231{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4833eff3{/static,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4694f434{/,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@56928e17{/api,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@dd2856e{/jobs/job/kill,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5a49af50{/stages/stage/kill,null,AVAILABLE}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@4baed682{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 17:59:16 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @41192ms
2018-12-26 17:59:16 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 17:59:16 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63969.
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:63969
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 63969, None)
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:63969 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 63969, None)
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 63969, None)
2018-12-26 17:59:17 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 63969, None)
2018-12-26 17:59:17 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3596b249{/metrics/json,null,AVAILABLE}
2018-12-26 18:00:15 [ WARN ] - [ org.apache.spark.internal.Logging$class.logWarning(Logging.scala:66) ] Ignored failure: java.util.concurrent.TimeoutException: Cannot receive any reply in 10 seconds
2018-12-26 18:00:15 [ WARN ] - [ org.apache.spark.internal.Logging$class.logWarning(Logging.scala:87) ] Error sending message [message = Heartbeat(driver,[Lscala.Tuple2;@13459487,BlockManagerId(driver, 192.168.0.112, 63969, None))] in 1 attempts
org.apache.spark.rpc.RpcTimeoutException: Futures timed out after [10 seconds]. This timeout is controlled by spark.executor.heartbeatInterval
	at org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:48)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:63)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$reportHeartBeat(Executor.scala:538)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply$mcV$sp(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.executor.Executor$$anon$1$$anonfun$run$1.apply(Executor.scala:567)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1951)
	at org.apache.spark.executor.Executor$$anon$1.run(Executor.scala:567)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.runAndReset$$$capture(FutureTask.java:308)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [10 seconds]
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.result(package.scala:190)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:81)
	... 15 more
2018-12-26 18:00:55 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 18:00:55 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545818455000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-26 18:01:02 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167e9e2b2d80007, negotiated timeout = 10000
2018-12-26 18:01:02 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-26 18:01:02 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167e9e2b2d80007 closed
2018-12-26 18:01:02 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167e9e2b2d80007
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 18:01:03 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 18:01:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 64019.
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-ac3bec54-3b3f-416f-bce7-c7c6cef53dc6
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @3690ms
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/jobs/job,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/jobs/job/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/stage,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/stage/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/stages/pool,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/stages/pool/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/storage/rdd,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/storage/rdd/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/environment,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/environment/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/executors/threadDump,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/static,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/api,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@34448e6c{/jobs/job/kill,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@60e9df3c{/stages/stage/kill,null,AVAILABLE}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@2d9278cd{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @3805ms
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64032.
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:64032
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 64032, None)
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:64032 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 64032, None)
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 64032, None)
2018-12-26 18:01:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 64032, None)
2018-12-26 18:01:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@59942b48{/metrics/json,null,AVAILABLE}
2018-12-26 18:01:10 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 18:01:10 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545818470000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-26 18:04:13 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-26 18:04:13 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-26 18:04:14 [ ERROR ] - [ org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:201) ] Connection timed out for connection string (127.0.0.1:2181) and timeout (1000) / elapsed (1242)
org.apache.curator.CuratorConnectionLossException: KeeperErrorCode = ConnectionLoss
	at org.apache.curator.ConnectionState.checkTimeouts(ConnectionState.java:198)
	at org.apache.curator.ConnectionState.getZooKeeper(ConnectionState.java:88)
	at org.apache.curator.CuratorZookeeperClient.getZooKeeper(CuratorZookeeperClient.java:115)
	at org.apache.curator.framework.imps.CuratorFrameworkImpl.getZooKeeper(CuratorFrameworkImpl.java:457)
	at org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:172)
	at org.apache.curator.framework.imps.ExistsBuilderImpl$2.call(ExistsBuilderImpl.java:161)
	at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107)
	at org.apache.curator.framework.imps.ExistsBuilderImpl.pathInForeground(ExistsBuilderImpl.java:157)
	at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:148)
	at org.apache.curator.framework.imps.ExistsBuilderImpl.forPath(ExistsBuilderImpl.java:36)
	at com.zqg.StreamKafka.getOffset.GetTopicOffsetFromZookeeper.getConsumerOffsets(GetTopicOffsetFromZookeeper.java:31)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:39)
2018-12-26 18:04:23 [ WARN ] - [ org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1108) ] Client session timed out, have not heard from server in 10000ms for sessionid 0x0
2018-12-26 18:04:23 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x0 closed
2018-12-26 18:04:23 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x0
2018-12-26 18:04:24 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-26 18:04:25 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 64114.
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-9498ce3e-0b4a-42f6-b75b-f69062932b83
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @13444ms
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4b61d0c6{/jobs,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6f815e7f{/jobs/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@65e7f52a{/jobs/job,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@304b9f1a{/jobs/job/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@75699e35{/stages,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@107e5441{/stages/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4aeaadc1{/stages/stage,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@263558c9{/stages/stage/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1f14f20c{/stages/pool,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7daa61f3{/stages/pool/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@62315f22{/storage,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6e4ea0bd{/storage/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@56f2bbea{/storage/rdd,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78f9ed3e{/storage/rdd/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1059754c{/environment,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@b0964b2{/environment/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@48e7b3d2{/executors,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7f4037ed{/executors/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@24e8de5c{/executors/threadDump,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@64040287{/executors/threadDump/json,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@110844f6{/static,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6f89f665{/,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@df1cff6{/api,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4925f4f5{/jobs/job/kill,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ad926d3{/stages/stage/kill,null,AVAILABLE}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@3e521715{HTTP/1.1}{0.0.0.0:4040}
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @13553ms
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64127.
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:64127
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 64127, None)
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:64127 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 64127, None)
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 64127, None)
2018-12-26 18:04:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 64127, None)
2018-12-26 18:04:25 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1e6308a9{/metrics/json,null,AVAILABLE}
2018-12-26 18:04:30 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-26 18:04:30 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545818670000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-27 09:39:59 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-27 09:39:59 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-27 09:40:00 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-27 09:40:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:40:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:40:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0002, negotiated timeout = 10000
2018-12-27 09:40:00 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:40:00 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:40:01 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0002 closed
2018-12-27 09:40:01 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0002
2018-12-27 09:40:02 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-27 09:40:03 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-27 09:40:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-27 09:40:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-27 09:40:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-27 09:40:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-27 09:40:03 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 65152.
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-80c6addc-3e07-47c2-b4ea-453376d014ca
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @6986ms
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7d0332e1{/jobs,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7a356a0d{/jobs/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs/job,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/job/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/stages,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/stages/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages/stage,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/stage/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/pool,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/pool/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/storage,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/storage/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage/rdd,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/rdd/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/environment,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/environment/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/executors,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/executors/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors/threadDump,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/threadDump/json,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/static,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/api,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/jobs/job/kill,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/stages/stage/kill,null,AVAILABLE}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@17f06475{HTTP/1.1}{0.0.0.0:4040}
2018-12-27 09:40:04 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @7132ms
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-27 09:40:04 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65165.
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:65165
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 65165, None)
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:65165 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 65165, None)
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 65165, None)
2018-12-27 09:40:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 65165, None)
2018-12-27 09:40:05 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@59712875{/metrics/json,null,AVAILABLE}
2018-12-27 09:40:10 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-27 09:40:10 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545874810000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-27 09:43:40 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-27 09:43:40 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@2034b64c
2018-12-27 09:43:41 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:43:41 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:43:41 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0005, negotiated timeout = 10000
2018-12-27 09:43:41 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:43:41 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:43:42 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0005 closed
2018-12-27 09:43:42 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0005
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-27 09:43:43 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:67)
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 65279.
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-a95bb5ed-9ca0-453b-bd05-8b9542650c43
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-27 09:43:43 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @4317ms
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/jobs,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/jobs/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/jobs/job,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/jobs/job/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/stages/stage,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/stages/stage/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/stages/pool,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/stages/pool/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/storage,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/storage/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/storage/rdd,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/storage/rdd/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/environment,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/environment/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/executors,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/executors/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/executors/threadDump,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/executors/threadDump/json,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/static,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@78b236a0{/,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@261d8190{/api,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@34448e6c{/jobs/job/kill,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@60e9df3c{/stages/stage/kill,null,AVAILABLE}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@32fdec40{HTTP/1.1}{0.0.0.0:4040}
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @4461ms
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65292.
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:65292
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 65292, None)
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:65292 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 65292, None)
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 65292, None)
2018-12-27 09:43:44 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 65292, None)
2018-12-27 09:43:44 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1869f114{/metrics/json,null,AVAILABLE}
2018-12-27 09:43:45 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:70) ] ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
2018-12-27 09:43:45 [ ERROR ] - [ org.apache.spark.internal.Logging$class.logError(Logging.scala:91) ] Error generating jobs for time 1545875025000 ms
org.apache.spark.SparkException: ArrayBuffer(org.apache.spark.SparkException: Couldn't find leader offsets for Set())
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.latestLeaderOffsets(DirectKafkaInputDStream.scala:133)
	at org.apache.spark.streaming.kafka.DirectKafkaInputDStream.compute(DirectKafkaInputDStream.scala:158)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.TransformedDStream$$anonfun$6.apply(TransformedDStream.scala:42)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.immutable.List.map(List.scala:285)
	at org.apache.spark.streaming.dstream.TransformedDStream.compute(TransformedDStream.scala:42)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.TransformedDStream.createRDDWithLocalProperties(TransformedDStream.scala:65)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.FlatMappedDStream.compute(FlatMappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.MappedDStream.compute(MappedDStream.scala:36)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1$$anonfun$apply$7.apply(DStream.scala:341)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1$$anonfun$1.apply(DStream.scala:340)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:415)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:335)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$getOrCompute$1.apply(DStream.scala:333)
	at scala.Option.orElse(Option.scala:289)
	at org.apache.spark.streaming.dstream.DStream.getOrCompute(DStream.scala:330)
	at org.apache.spark.streaming.dstream.ForEachDStream.generateJob(ForEachDStream.scala:48)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:117)
	at org.apache.spark.streaming.DStreamGraph$$anonfun$1.apply(DStreamGraph.scala:116)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)
	at org.apache.spark.streaming.DStreamGraph.generateJobs(DStreamGraph.scala:116)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:249)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anonfun$3.apply(JobGenerator.scala:247)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.streaming.scheduler.JobGenerator.generateJobs(JobGenerator.scala:247)
	at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:183)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:89)
	at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:88)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
2018-12-27 09:49:53 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@793f29ff
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:49:53 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0006, negotiated timeout = 10000
2018-12-27 09:49:53 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:49:53 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:49:54 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0006 closed
2018-12-27 09:49:54 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0006
2018-12-27 09:49:55 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-27 09:49:56 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.SparkStreamingDirect.getStreamingContext(SparkStreamingDirect.java:38)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:62)
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 65377.
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-19e0e70e-9263-46a0-8e76-77c49a7e5124
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @4330ms
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3569edd5{/jobs,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1f651cd8{/jobs/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7d0332e1{/jobs/job,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7a356a0d{/jobs/job/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@c827db{/stages,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@377c68c6{/stages/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@538cd0f2{/stages/stage,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@238ad8c{/stages/stage/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@430fa4ef{/stages/pool,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1761de10{/stages/pool/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@22df874e{/storage,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@654c1a54{/storage/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5bdaf2ce{/storage/rdd,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42d236fb{/storage/rdd/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1ce93c18{/environment,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f21b6b{/environment/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1532c619{/executors,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@46044faa{/executors/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1358b28e{/executors/threadDump,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1a78dacd{/executors/threadDump/json,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@19f9d595{/static,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7de4a01f{/,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@2bfeb1ef{/api,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@778ca8ef{/jobs/job/kill,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@208e9ef6{/stages/stage/kill,null,AVAILABLE}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@44f82b6d{HTTP/1.1}{0.0.0.0:4040}
2018-12-27 09:49:56 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @4464ms
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-27 09:49:56 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65398.
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:65398
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 65398, None)
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:65398 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 65398, None)
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 65398, None)
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 65398, None)
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@72ed9aad{/metrics/json,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Slide time = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Storage level = Serialized 1x Replicated
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Checkpoint interval = null
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Remember interval = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@970a121
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Slide time = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Storage level = Serialized 1x Replicated
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Checkpoint interval = null
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Remember interval = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@7b92de51
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Slide time = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Storage level = Serialized 1x Replicated
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Checkpoint interval = null
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Remember interval = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized and validated org.apache.spark.streaming.kafka.DirectKafkaInputDStream@970a121
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Slide time = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Storage level = Serialized 1x Replicated
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Checkpoint interval = null
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Remember interval = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized and validated org.apache.spark.streaming.dstream.TransformedDStream@4dce4d6e
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Slide time = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Storage level = Serialized 1x Replicated
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Checkpoint interval = null
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Remember interval = 5000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@4da21870
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Started timer for JobGenerator at time 1545875400000
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Started JobGenerator at 1545875400000 ms
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Started JobScheduler
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@36b310aa{/streaming,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3874b815{/streaming/json,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1816e24a{/streaming/batch,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@47b179d7{/streaming/batch/json,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3402b4c9{/static/streaming,null,AVAILABLE}
2018-12-27 09:49:57 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] StreamingContext started
2018-12-27 09:50:00 [ INFO ] - [ kafka.utils.Logging$class.info(Logging.scala:68) ] Verifying properties
2018-12-27 09:50:00 [ INFO ] - [ kafka.utils.Logging$class.info(Logging.scala:68) ] Property group.id is overridden to 
2018-12-27 09:50:00 [ INFO ] - [ kafka.utils.Logging$class.info(Logging.scala:68) ] Property zookeeper.connect is overridden to 
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875400000 ms
2018-12-27 09:50:00 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875400000 ms.0 from job set of time 1545875400000 ms
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@22bf7e7e
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0007, negotiated timeout = 10000
2018-12-27 09:50:00 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:00 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0007 closed
2018-12-27 09:50:00 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0007
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875400000 ms.0 from job set of time 1545875400000 ms
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875400000 ms.1 from job set of time 1545875400000 ms
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875400000 ms.1 from job set of time 1545875400000 ms
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.282 s for time 1545875400000 ms (execution: 0.191 s)
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875405000 ms
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875405000 ms.0 from job set of time 1545875405000 ms
2018-12-27 09:50:05 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@3dc4e6ea
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0008, negotiated timeout = 10000
2018-12-27 09:50:05 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:05 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0008 closed
2018-12-27 09:50:05 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0008
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875405000 ms.0 from job set of time 1545875405000 ms
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875405000 ms.1 from job set of time 1545875405000 ms
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875405000 ms.1 from job set of time 1545875405000 ms
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.077 s for time 1545875405000 ms (execution: 0.067 s)
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 0 from persistence list
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 0 from persistence list
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 0
2018-12-27 09:50:05 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 0
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875410000 ms
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875410000 ms.0 from job set of time 1545875410000 ms
2018-12-27 09:50:10 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@35410b0b
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0009, negotiated timeout = 10000
2018-12-27 09:50:10 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:10 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0009 closed
2018-12-27 09:50:10 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0009
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875410000 ms.0 from job set of time 1545875410000 ms
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875410000 ms.1 from job set of time 1545875410000 ms
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875410000 ms.1 from job set of time 1545875410000 ms
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.076 s for time 1545875410000 ms (execution: 0.069 s)
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 1 from persistence list
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 1
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 1 from persistence list
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 1
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:10 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 1545875400000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875415000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875415000 ms.0 from job set of time 1545875415000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@bfe7472
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c000a, negotiated timeout = 10000
2018-12-27 09:50:15 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:15 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c000a closed
2018-12-27 09:50:15 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c000a
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875415000 ms.0 from job set of time 1545875415000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875415000 ms.1 from job set of time 1545875415000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875415000 ms.1 from job set of time 1545875415000 ms
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 2 from persistence list
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.067 s for time 1545875415000 ms (execution: 0.061 s)
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 2
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 2 from persistence list
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 2
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:15 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 1545875405000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875420000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875420000 ms.0 from job set of time 1545875420000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@22676831
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c000b, negotiated timeout = 10000
2018-12-27 09:50:20 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:20 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c000b closed
2018-12-27 09:50:20 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c000b
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875420000 ms.0 from job set of time 1545875420000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875420000 ms.1 from job set of time 1545875420000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875420000 ms.1 from job set of time 1545875420000 ms
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 3 from persistence list
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.076 s for time 1545875420000 ms (execution: 0.063 s)
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 3
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 3 from persistence list
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 3
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:20 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 1545875410000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Added jobs for time 1545875425000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875425000 ms.0 from job set of time 1545875425000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@49d1ec8d
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c000c, negotiated timeout = 10000
2018-12-27 09:50:25 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:50:25 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c000c closed
2018-12-27 09:50:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c000c
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875425000 ms.0 from job set of time 1545875425000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting job streaming job 1545875425000 ms.1 from job set of time 1545875425000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Finished job streaming job 1545875425000 ms.1 from job set of time 1545875425000 ms
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 4 from persistence list
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Total delay: 0.076 s for time 1545875425000 ms (execution: 0.063 s)
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 4
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 4 from persistence list
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Removing RDD 4
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Deleting batches: 
2018-12-27 09:50:25 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] remove old batch metadata: 1545875415000 ms
2018-12-27 09:52:58 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-27 09:52:58 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@793f29ff
2018-12-27 09:52:59 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:52:59 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:52:59 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c000e, negotiated timeout = 10000
2018-12-27 09:52:59 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:52:59 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:52:59 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c000e closed
2018-12-27 09:52:59 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c000e
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-27 09:53:00 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.SparkStreamingDirect.getStreamingContext(SparkStreamingDirect.java:38)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:62)
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 65483.
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-4f296eb3-af45-46ab-8308-decddcabae9c
2018-12-27 09:53:00 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @3456ms
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@75504cef{/jobs,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6c8a68c1{/jobs/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@56193c7d{/jobs/job,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@28c88600{/jobs/job/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5f8890c2{/stages,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@607b2792{/stages/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7f9e1534{/stages/stage,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@138a7441{/stages/stage/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@81ff872{/stages/pool,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@31611954{/stages/pool/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3e598df9{/storage,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7e31ce0f{/storage/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@99a65d3{/storage/rdd,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3088660d{/storage/rdd/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@42cc13a0{/environment,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@32fdec40{/environment/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6813a331{/executors,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1bd81830{/executors/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@39ab59f8{/executors/threadDump,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@64e92d61{/executors/threadDump/json,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@111610e6{/static,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4ad4936c{/,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@29d37757{/api,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@4fcc529{/jobs/job/kill,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@25cc7470{/stages/stage/kill,null,AVAILABLE}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@5e6a6afb{HTTP/1.1}{0.0.0.0:4040}
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @3605ms
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 65504.
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:65504
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 65504, None)
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:65504 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 65504, None)
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 65504, None)
2018-12-27 09:53:01 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 65504, None)
2018-12-27 09:53:01 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@21ab988f{/metrics/json,null,AVAILABLE}
2018-12-27 09:53:05 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:10 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:15 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:25 [ INFO ] - [ org.apache.curator.framework.imps.CuratorFrameworkImpl.start(CuratorFrameworkImpl.java:223) ] Starting
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:zookeeper.version=3.4.10-39d3a4f269333c922ed3db283be479f9deacaa0f, built on 03/23/2017 10:13 GMT
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:host.name=windows10.microdone.cn
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.version=1.8.0_171
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.vendor=Oracle Corporation
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.home=C:\Program Files\Java\jdk1.8.0_171\jre
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.class.path=C:\Program Files\Java\jdk1.8.0_171\jre\lib\charsets.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\deploy.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\access-bridge-64.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\cldrdata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\dnsns.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jaccess.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\jfxrt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\localedata.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\nashorn.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunec.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunjce_provider.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunmscapi.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\sunpkcs11.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\ext\zipfs.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\javaws.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jce.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfr.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jfxswt.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\jsse.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\management-agent.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\plugin.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\resources.jar;C:\Program Files\Java\jdk1.8.0_171\jre\lib\rt.jar;D:\springbootworkspace\spark\target\classes;D:\Repository\org\scala-lang\scala-library\2.11.8\scala-library-2.11.8.jar;D:\Repository\org\apache\spark\spark-core_2.11\2.1.0\spark-core_2.11-2.1.0.jar;D:\Repository\org\apache\avro\avro-mapred\1.7.7\avro-mapred-1.7.7-hadoop2.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7.jar;D:\Repository\org\apache\avro\avro-ipc\1.7.7\avro-ipc-1.7.7-tests.jar;D:\Repository\org\codehaus\jackson\jackson-core-asl\1.9.13\jackson-core-asl-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-mapper-asl\1.9.13\jackson-mapper-asl-1.9.13.jar;D:\Repository\com\twitter\chill_2.11\0.8.0\chill_2.11-0.8.0.jar;D:\Repository\com\esotericsoftware\kryo-shaded\3.0.3\kryo-shaded-3.0.3.jar;D:\Repository\com\esotericsoftware\minlog\1.3.0\minlog-1.3.0.jar;D:\Repository\org\objenesis\objenesis\2.1\objenesis-2.1.jar;D:\Repository\com\twitter\chill-java\0.8.0\chill-java-0.8.0.jar;D:\Repository\org\apache\xbean\xbean-asm5-shaded\4.4\xbean-asm5-shaded-4.4.jar;D:\Repository\org\apache\spark\spark-launcher_2.11\2.1.0\spark-launcher_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-network-common_2.11\2.1.0\spark-network-common_2.11-2.1.0.jar;D:\Repository\org\fusesource\leveldbjni\leveldbjni-all\1.8\leveldbjni-all-1.8.jar;D:\Repository\com\fasterxml\jackson\core\jackson-annotations\2.6.5\jackson-annotations-2.6.5.jar;D:\Repository\org\apache\spark\spark-network-shuffle_2.11\2.1.0\spark-network-shuffle_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-unsafe_2.11\2.1.0\spark-unsafe_2.11-2.1.0.jar;D:\Repository\net\java\dev\jets3t\jets3t\0.7.1\jets3t-0.7.1.jar;D:\Repository\commons-codec\commons-codec\1.3\commons-codec-1.3.jar;D:\Repository\commons-httpclient\commons-httpclient\3.1\commons-httpclient-3.1.jar;D:\Repository\org\apache\curator\curator-recipes\2.4.0\curator-recipes-2.4.0.jar;D:\Repository\org\apache\curator\curator-framework\2.4.0\curator-framework-2.4.0.jar;D:\Repository\com\google\guava\guava\14.0.1\guava-14.0.1.jar;D:\Repository\javax\servlet\javax.servlet-api\3.1.0\javax.servlet-api-3.1.0.jar;D:\Repository\org\apache\commons\commons-lang3\3.5\commons-lang3-3.5.jar;D:\Repository\org\apache\commons\commons-math3\3.4.1\commons-math3-3.4.1.jar;D:\Repository\com\google\code\findbugs\jsr305\1.3.9\jsr305-1.3.9.jar;D:\Repository\org\slf4j\slf4j-api\1.7.16\slf4j-api-1.7.16.jar;D:\Repository\org\slf4j\jul-to-slf4j\1.7.16\jul-to-slf4j-1.7.16.jar;D:\Repository\org\slf4j\jcl-over-slf4j\1.7.16\jcl-over-slf4j-1.7.16.jar;D:\Repository\log4j\log4j\1.2.17\log4j-1.2.17.jar;D:\Repository\org\slf4j\slf4j-log4j12\1.7.16\slf4j-log4j12-1.7.16.jar;D:\Repository\com\ning\compress-lzf\1.0.3\compress-lzf-1.0.3.jar;D:\Repository\org\xerial\snappy\snappy-java\1.1.2.6\snappy-java-1.1.2.6.jar;D:\Repository\net\jpountz\lz4\lz4\1.3.0\lz4-1.3.0.jar;D:\Repository\org\roaringbitmap\RoaringBitmap\0.5.11\RoaringBitmap-0.5.11.jar;D:\Repository\commons-net\commons-net\2.2\commons-net-2.2.jar;D:\Repository\org\json4s\json4s-jackson_2.11\3.2.11\json4s-jackson_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-core_2.11\3.2.11\json4s-core_2.11-3.2.11.jar;D:\Repository\org\json4s\json4s-ast_2.11\3.2.11\json4s-ast_2.11-3.2.11.jar;D:\Repository\com\thoughtworks\paranamer\paranamer\2.6\paranamer-2.6.jar;D:\Repository\org\scala-lang\scalap\2.11.0\scalap-2.11.0.jar;D:\Repository\org\scala-lang\scala-compiler\2.11.0\scala-compiler-2.11.0.jar;D:\Repository\org\glassfish\jersey\core\jersey-client\2.22.2\jersey-client-2.22.2.jar;D:\Repository\javax\ws\rs\javax.ws.rs-api\2.0.1\javax.ws.rs-api-2.0.1.jar;D:\Repository\org\glassfish\hk2\hk2-api\2.4.0-b34\hk2-api-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-utils\2.4.0-b34\hk2-utils-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\aopalliance-repackaged\2.4.0-b34\aopalliance-repackaged-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\external\javax.inject\2.4.0-b34\javax.inject-2.4.0-b34.jar;D:\Repository\org\glassfish\hk2\hk2-locator\2.4.0-b34\hk2-locator-2.4.0-b34.jar;D:\Repository\org\javassist\javassist\3.18.1-GA\javassist-3.18.1-GA.jar;D:\Repository\org\glassfish\jersey\core\jersey-common\2.22.2\jersey-common-2.22.2.jar;D:\Repository\javax\annotation\javax.annotation-api\1.2\javax.annotation-api-1.2.jar;D:\Repository\org\glassfish\jersey\bundles\repackaged\jersey-guava\2.22.2\jersey-guava-2.22.2.jar;D:\Repository\org\glassfish\hk2\osgi-resource-locator\1.0.1\osgi-resource-locator-1.0.1.jar;D:\Repository\org\glassfish\jersey\core\jersey-server\2.22.2\jersey-server-2.22.2.jar;D:\Repository\org\glassfish\jersey\media\jersey-media-jaxb\2.22.2\jersey-media-jaxb-2.22.2.jar;D:\Repository\javax\validation\validation-api\1.1.0.Final\validation-api-1.1.0.Final.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet\2.22.2\jersey-container-servlet-2.22.2.jar;D:\Repository\org\glassfish\jersey\containers\jersey-container-servlet-core\2.22.2\jersey-container-servlet-core-2.22.2.jar;D:\Repository\io\netty\netty-all\4.0.42.Final\netty-all-4.0.42.Final.jar;D:\Repository\io\netty\netty\3.8.0.Final\netty-3.8.0.Final.jar;D:\Repository\com\clearspring\analytics\stream\2.7.0\stream-2.7.0.jar;D:\Repository\io\dropwizard\metrics\metrics-core\3.1.2\metrics-core-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-jvm\3.1.2\metrics-jvm-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-json\3.1.2\metrics-json-3.1.2.jar;D:\Repository\io\dropwizard\metrics\metrics-graphite\3.1.2\metrics-graphite-3.1.2.jar;D:\Repository\com\fasterxml\jackson\core\jackson-databind\2.6.5\jackson-databind-2.6.5.jar;D:\Repository\com\fasterxml\jackson\core\jackson-core\2.6.5\jackson-core-2.6.5.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-scala_2.11\2.6.5\jackson-module-scala_2.11-2.6.5.jar;D:\Repository\org\scala-lang\scala-reflect\2.11.7\scala-reflect-2.11.7.jar;D:\Repository\com\fasterxml\jackson\module\jackson-module-paranamer\2.6.5\jackson-module-paranamer-2.6.5.jar;D:\Repository\org\apache\ivy\ivy\2.4.0\ivy-2.4.0.jar;D:\Repository\oro\oro\2.0.8\oro-2.0.8.jar;D:\Repository\net\razorvine\pyrolite\4.13\pyrolite-4.13.jar;D:\Repository\net\sf\py4j\py4j\0.10.4\py4j-0.10.4.jar;D:\Repository\org\apache\spark\spark-tags_2.11\2.1.0\spark-tags_2.11-2.1.0.jar;D:\Repository\org\scalatest\scalatest_2.11\2.2.6\scalatest_2.11-2.2.6.jar;D:\Repository\org\apache\commons\commons-crypto\1.0.0\commons-crypto-1.0.0.jar;D:\Repository\org\spark-project\spark\unused\1.0.0\unused-1.0.0.jar;D:\Repository\org\apache\spark\spark-streaming_2.11\2.1.0\spark-streaming_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-sql_2.11\2.1.0\spark-sql_2.11-2.1.0.jar;D:\Repository\com\univocity\univocity-parsers\2.2.1\univocity-parsers-2.2.1.jar;D:\Repository\org\apache\spark\spark-sketch_2.11\2.1.0\spark-sketch_2.11-2.1.0.jar;D:\Repository\org\apache\spark\spark-catalyst_2.11\2.1.0\spark-catalyst_2.11-2.1.0.jar;D:\Repository\org\codehaus\janino\janino\3.0.0\janino-3.0.0.jar;D:\Repository\org\codehaus\janino\commons-compiler\3.0.0\commons-compiler-3.0.0.jar;D:\Repository\org\antlr\antlr4-runtime\4.5.3\antlr4-runtime-4.5.3.jar;D:\Repository\org\apache\parquet\parquet-column\1.8.1\parquet-column-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-common\1.8.1\parquet-common-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-encoding\1.8.1\parquet-encoding-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-hadoop\1.8.1\parquet-hadoop-1.8.1.jar;D:\Repository\org\apache\parquet\parquet-format\2.3.0-incubating\parquet-format-2.3.0-incubating.jar;D:\Repository\org\apache\parquet\parquet-jackson\1.8.1\parquet-jackson-1.8.1.jar;D:\Repository\org\apache\hadoop\hadoop-client\2.6.5\hadoop-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-common\2.6.5\hadoop-common-2.6.5.jar;D:\Repository\commons-cli\commons-cli\1.2\commons-cli-1.2.jar;D:\Repository\xmlenc\xmlenc\0.52\xmlenc-0.52.jar;D:\Repository\commons-io\commons-io\2.4\commons-io-2.4.jar;D:\Repository\commons-collections\commons-collections\3.2.2\commons-collections-3.2.2.jar;D:\Repository\commons-logging\commons-logging\1.1.3\commons-logging-1.1.3.jar;D:\Repository\commons-lang\commons-lang\2.6\commons-lang-2.6.jar;D:\Repository\commons-configuration\commons-configuration\1.6\commons-configuration-1.6.jar;D:\Repository\commons-digester\commons-digester\1.8\commons-digester-1.8.jar;D:\Repository\commons-beanutils\commons-beanutils\1.7.0\commons-beanutils-1.7.0.jar;D:\Repository\commons-beanutils\commons-beanutils-core\1.8.0\commons-beanutils-core-1.8.0.jar;D:\Repository\org\apache\avro\avro\1.7.4\avro-1.7.4.jar;D:\Repository\com\google\protobuf\protobuf-java\2.5.0\protobuf-java-2.5.0.jar;D:\Repository\com\google\code\gson\gson\2.2.4\gson-2.2.4.jar;D:\Repository\org\apache\hadoop\hadoop-auth\2.6.5\hadoop-auth-2.6.5.jar;D:\Repository\org\apache\httpcomponents\httpclient\4.2.5\httpclient-4.2.5.jar;D:\Repository\org\apache\httpcomponents\httpcore\4.2.4\httpcore-4.2.4.jar;D:\Repository\org\apache\directory\server\apacheds-kerberos-codec\2.0.0-M15\apacheds-kerberos-codec-2.0.0-M15.jar;D:\Repository\org\apache\directory\server\apacheds-i18n\2.0.0-M15\apacheds-i18n-2.0.0-M15.jar;D:\Repository\org\apache\directory\api\api-asn1-api\1.0.0-M20\api-asn1-api-1.0.0-M20.jar;D:\Repository\org\apache\directory\api\api-util\1.0.0-M20\api-util-1.0.0-M20.jar;D:\Repository\org\apache\curator\curator-client\2.6.0\curator-client-2.6.0.jar;D:\Repository\org\htrace\htrace-core\3.0.4\htrace-core-3.0.4.jar;D:\Repository\org\apache\commons\commons-compress\1.4.1\commons-compress-1.4.1.jar;D:\Repository\org\tukaani\xz\1.0\xz-1.0.jar;D:\Repository\org\apache\hadoop\hadoop-hdfs\2.6.5\hadoop-hdfs-2.6.5.jar;D:\Repository\org\mortbay\jetty\jetty-util\6.1.26\jetty-util-6.1.26.jar;D:\Repository\xerces\xercesImpl\2.9.1\xercesImpl-2.9.1.jar;D:\Repository\xml-apis\xml-apis\1.3.04\xml-apis-1.3.04.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-app\2.6.5\hadoop-mapreduce-client-app-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-common\2.6.5\hadoop-mapreduce-client-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-client\2.6.5\hadoop-yarn-client-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-server-common\2.6.5\hadoop-yarn-server-common-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-shuffle\2.6.5\hadoop-mapreduce-client-shuffle-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-api\2.6.5\hadoop-yarn-api-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-core\2.6.5\hadoop-mapreduce-client-core-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-yarn-common\2.6.5\hadoop-yarn-common-2.6.5.jar;D:\Repository\javax\xml\bind\jaxb-api\2.2.2\jaxb-api-2.2.2.jar;D:\Repository\javax\xml\stream\stax-api\1.0-2\stax-api-1.0-2.jar;D:\Repository\javax\activation\activation\1.1\activation-1.1.jar;D:\Repository\javax\servlet\servlet-api\2.5\servlet-api-2.5.jar;D:\Repository\com\sun\jersey\jersey-core\1.9\jersey-core-1.9.jar;D:\Repository\com\sun\jersey\jersey-client\1.9\jersey-client-1.9.jar;D:\Repository\org\codehaus\jackson\jackson-jaxrs\1.9.13\jackson-jaxrs-1.9.13.jar;D:\Repository\org\codehaus\jackson\jackson-xc\1.9.13\jackson-xc-1.9.13.jar;D:\Repository\org\apache\hadoop\hadoop-mapreduce-client-jobclient\2.6.5\hadoop-mapreduce-client-jobclient-2.6.5.jar;D:\Repository\org\apache\hadoop\hadoop-annotations\2.6.5\hadoop-annotations-2.6.5.jar;D:\Repository\org\apache\kafka\kafka-clients\0.8.2.2\kafka-clients-0.8.2.2.jar;D:\Repository\org\apache\kafka\kafka_2.11\0.8.2.2\kafka_2.11-0.8.2.2.jar;D:\Repository\org\scala-lang\modules\scala-xml_2.11\1.0.2\scala-xml_2.11-1.0.2.jar;D:\Repository\com\yammer\metrics\metrics-core\2.2.0\metrics-core-2.2.0.jar;D:\Repository\net\sf\jopt-simple\jopt-simple\3.2\jopt-simple-3.2.jar;D:\Repository\org\scala-lang\modules\scala-parser-combinators_2.11\1.0.2\scala-parser-combinators_2.11-1.0.2.jar;D:\Repository\com\101tec\zkclient\0.3\zkclient-0.3.jar;D:\Repository\mysql\mysql-connector-java\5.1.38\mysql-connector-java-5.1.38.jar;D:\Repository\org\apache\spark\spark-streaming-kafka-0-8_2.11\2.1.0\spark-streaming-kafka-0-8_2.11-2.1.0.jar;D:\Repository\org\apache\zookeeper\zookeeper\3.4.10\zookeeper-3.4.10.jar;D:\Repository\jline\jline\0.9.94\jline-0.9.94.jar;D:\developTools\IntelliJ IDEA 2018.2.3\lib\idea_rt.jar
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.library.path=C:\Program Files\Java\jdk1.8.0_171\bin;C:\WINDOWS\Sun\Java\bin;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\system32;C:\WINDOWS;C:\WINDOWS\System32\Wbem;C:\WINDOWS\System32\WindowsPowerShell\v1.0\;D:\developTools\Git\cmd;D:\developTools\svn\bin;C:\WINDOWS\System32\OpenSSH\;C:\Program Files (x86)\scala\bin;C:\Users\junmei02\AppData\Local\Microsoft\WindowsApps;C:\Program Files\Java\jdk1.8.0_171\bin;D:\developTools\apache-maven-3.5.2\bin;D:\developTools\mysql-5.6.27-winx64\bin;D:\developTools\Git\cmd;D:\hadoop\hadoop-common-2.2.0-bin-master\bin;D:\hadoop\hbase-1.2.6\bin;C:\Program Files (x86)\scala\bin;D:\bigdata\zookeeper-3.4.10\bin;;.
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.io.tmpdir=C:\Users\junmei02\AppData\Local\Temp\
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:java.compiler=<NA>
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.name=Windows 10
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.arch=amd64
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:os.version=10.0
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.name=junmei02
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.home=C:\Users\junmei02
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.Environment.logEnv(Environment.java:100) ] Client environment:user.dir=D:\springbootworkspace\spark
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.<init>(ZooKeeper.java:438) ] Initiating client connection, connectString=127.0.0.1:2181 sessionTimeout=10000 watcher=org.apache.curator.ConnectionState@793f29ff
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.logStartConnect(ClientCnxn.java:1032) ] Opening socket connection to server 127.0.0.1/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.primeConnection(ClientCnxn.java:876) ] Socket connection established to 127.0.0.1/127.0.0.1:2181, initiating session
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$SendThread.onConnected(ClientCnxn.java:1299) ] Session establishment complete on server 127.0.0.1/127.0.0.1:2181, sessionid = 0x167ed477b3c0012, negotiated timeout = 10000
2018-12-27 09:53:25 [ INFO ] - [ org.apache.curator.framework.state.ConnectionStateManager.postState(ConnectionStateManager.java:194) ] State change: CONNECTED
2018-12-27 09:53:25 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ZooKeeper.close(ZooKeeper.java:684) ] Session: 0x167ed477b3c0012 closed
2018-12-27 09:53:25 [ INFO ] - [ org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:519) ] EventThread shut down for session: 0x167ed477b3c0012
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Running Spark version 2.1.0
2018-12-27 09:53:26 [ ERROR ] - [ org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:396) ] Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:378)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:393)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:386)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:116)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:93)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:73)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:293)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:283)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:260)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:789)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:774)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:647)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2373)
	at scala.Option.getOrElse(Option.scala:121)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2373)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:295)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:837)
	at org.apache.spark.streaming.StreamingContext.<init>(StreamingContext.scala:84)
	at org.apache.spark.streaming.api.java.JavaStreamingContext.<init>(JavaStreamingContext.scala:138)
	at com.zqg.StreamKafka.SparkStreamingDirect.getStreamingContext(SparkStreamingDirect.java:38)
	at com.zqg.StreamKafka.KafkaSparkStreamingWithZookeeperoffset.main(KafkaSparkStreamingWithZookeeperoffset.java:62)
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls to: junmei02
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls to: junmei02
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing view acls groups to: 
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Changing modify acls groups to: 
2018-12-27 09:53:26 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(junmei02); groups with view permissions: Set(); users  with modify permissions: Set(junmei02); groups with modify permissions: Set()
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'sparkDriver' on port 49166.
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering MapOutputTracker
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManagerMaster
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] BlockManagerMasterEndpoint up
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Created local directory at C:\Users\junmei02\AppData\Local\Temp\blockmgr-885ab339-4dba-4565-99c2-25708d79cfdf
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] MemoryStore started with capacity 900.6 MB
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering OutputCommitCoordinator
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.util.log.Log.initialized(Log.java:186) ] Logging initialized @3359ms
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:327) ] jetty-9.2.z-SNAPSHOT
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@d59970a{/jobs,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@1e411d81{/jobs/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@53b98ff6{/jobs/job,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3e6fd0b9{/jobs/job/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7fcff1b9{/stages,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@697446d4{/stages/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@76adb233{/stages/stage,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@36074e47{/stages/stage/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@36453307{/stages/pool,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7dcc91fd{/stages/pool/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@66eb985d{/storage,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6a9287b1{/storage/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@75504cef{/storage/rdd,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@6c8a68c1{/storage/rdd/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@56193c7d{/environment,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@28c88600{/environment/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@5f8890c2{/executors,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@607b2792{/executors/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7f9e1534{/executors/threadDump,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@138a7441{/executors/threadDump/json,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@81ff872{/static,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@31611954{/,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@3e598df9{/api,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@7e31ce0f{/jobs/job/kill,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@99a65d3{/stages/stage/kill,null,AVAILABLE}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.AbstractConnector.doStart(AbstractConnector.java:266) ] Started ServerConnector@58faa93b{HTTP/1.1}{0.0.0.0:4040}
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.Server.doStart(Server.java:379) ] Started @3519ms
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'SparkUI' on port 4040.
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Bound SparkUI to 0.0.0.0, and started at http://192.168.0.112:4040
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Starting executor ID driver on host localhost
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49187.
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Server created on 192.168.0.112:49187
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering BlockManager BlockManagerId(driver, 192.168.0.112, 49187, None)
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registering block manager 192.168.0.112:49187 with 900.6 MB RAM, BlockManagerId(driver, 192.168.0.112, 49187, None)
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Registered BlockManager BlockManagerId(driver, 192.168.0.112, 49187, None)
2018-12-27 09:53:27 [ INFO ] - [ org.apache.spark.internal.Logging$class.logInfo(Logging.scala:54) ] Initialized BlockManager: BlockManagerId(driver, 192.168.0.112, 49187, None)
2018-12-27 09:53:27 [ INFO ] - [ org.spark_project.jetty.server.handler.ContextHandler.doStart(ContextHandler.java:744) ] Started o.s.j.s.ServletContextHandler@29314cc9{/metrics/json,null,AVAILABLE}
2018-12-27 09:53:30 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:35 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:40 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
2018-12-27 09:53:45 [ WARN ] - [ org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:212) ] There are no ConnectionStateListeners registered.
